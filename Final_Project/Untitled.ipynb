{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "409b7fb3-73aa-4b89-919d-a7a7872df215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/anaconda3/lib/python3.12/site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from xgboost) (1.13.1)\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "!pip install xgboost\n",
    "\n",
    "# Standard library imports\n",
    "import hashlib\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from category_encoders import TargetEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, PowerTransformer, StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Local module imports\n",
    "from clean_data_helper import *\n",
    "from feature_summary import export_feature_summary\n",
    "from plot_histograms import *\n",
    "from preprocessing_helper import *\n",
    "from round_to_nearest import round_to_nearest_multiple\n",
    "from split_dataset_by_missing_and_type import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f000026-b79e-4eb8-8d1d-2ca7d4b188e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 231)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('final_proj_data.csv')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2df0d8-bb44-4fe2-bcdf-3f7b6b158471",
   "metadata": {},
   "source": [
    "### Rough Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a640d758-b123-4e1b-bf0d-419f81829bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset shape after column removal: (10000, 67)\n",
      "New dataset shape after removing rows with more than 30% missing columns: (9080, 67)\n"
     ]
    }
   ],
   "source": [
    "cleaned_data = remove_columns_with_missing_values(data, 0.25)\n",
    "cleaned_data = remove_rows_with_missing_values(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dbe0e8f-1a11-4b25-98b4-0b2123bc9873",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df, categorical_df = split_dataset(cleaned_data)\n",
    "# numeric_df.to_csv('numeric_df.csv', index=False)\n",
    "# categorical_df.to_csv('categorical_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5576789-8b59-4217-a2a2-73a7e2fd2f24",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b74889-c102-4683-a366-67b1bfa66ac8",
   "metadata": {},
   "source": [
    "#### Numericals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e2c871e-6f10-4ee7-86b7-cfda9e91f5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature summary exported to num_sum_missing.csv\n"
     ]
    }
   ],
   "source": [
    "df_with_missing, df_without_missing = split_dataframe_by_missing_values(numeric_df)\n",
    "export_feature_summary(df_with_missing, 'num_sum_missing.csv', unique_threshold = 200)\n",
    "df_with_missing.to_csv('df_with_missing.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbd37324-bdc7-49e0-aaa3-377fff4f1af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result {'low_unique_features': ['Var7'], 'moderate_unique_features': ['Var65', 'Var144'], 'high_unique_features': ['Var6', 'Var13', 'Var21', 'Var24', 'Var74', 'Var81', 'Var109', 'Var119', 'Var125', 'Var140', 'Var149'], 'high_missing_features': []}\n"
     ]
    }
   ],
   "source": [
    "result = get_imputation_feature_lists_from_dataset(df_with_missing, missing_threshold=0.2)\n",
    "print(\"result\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11742bdd-189c-4011-8ff0-703cacfb0544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Var6  Var7   Var13  Var21  Var24  Var65  Var74      Var81  Var109  \\\n",
      "0   812.0  14.0  1252.0  156.0    0.0   27.0   14.0  227693.10    32.0   \n",
      "1  2688.0   7.0  8820.0  364.0    4.0    9.0  210.0   17662.35   112.0   \n",
      "2  1015.0  14.0  1784.0  136.0    2.0   18.0   98.0  190181.10    32.0   \n",
      "3   168.0   0.0     0.0   24.0    0.0    9.0    0.0  348843.00    16.0   \n",
      "4    14.0   0.0     0.0   36.0    0.0    9.0    0.0  235971.00     8.0   \n",
      "\n",
      "   Var119   Var125  Var140  Var144    Var149  \n",
      "0   525.0   4743.0   410.0    27.0       0.0  \n",
      "1  1065.0  44541.0    60.0     9.0  398034.0  \n",
      "2   625.0  14751.0  5720.0    27.0  554421.0  \n",
      "3   275.0      0.0     0.0     9.0       0.0  \n",
      "4    45.0      0.0     0.0     0.0       0.0  \n"
     ]
    }
   ],
   "source": [
    "# Assuming df_with_missing is a subset of another DataFrame `df`\n",
    "# Ensure you create a copy to avoid SettingWithCopyWarning\n",
    "df_with_missing = df_with_missing.copy()\n",
    "\n",
    "# Define feature groups\n",
    "low_unique_features = ['Var7']\n",
    "moderate_unique_features = ['Var65', 'Var144']\n",
    "high_unique_features = ['Var6', 'Var13', 'Var21', 'Var24', 'Var74', \n",
    "                        'Var81', 'Var109', 'Var119', 'Var125', \n",
    "                        'Var140', 'Var149']\n",
    "\n",
    "# Numerical variables with a common divisor of 7\n",
    "numerical_divisor_7_group = ['Var7']\n",
    "\n",
    "# Numerical variables with a common divisor of 9\n",
    "numerical_divisor_9_group = ['Var144', 'Var65']\n",
    "\n",
    "# Imputation for low unique features (Mode Imputation)\n",
    "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df_with_missing.loc[:, low_unique_features] = mode_imputer.fit_transform(\n",
    "    df_with_missing[low_unique_features]\n",
    ")\n",
    "\n",
    "# Imputation for moderate unique features (Median Imputation)\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "df_with_missing.loc[:, moderate_unique_features] = median_imputer.fit_transform(\n",
    "    df_with_missing[moderate_unique_features]\n",
    ")\n",
    "\n",
    "# Imputation for high unique features (Mean Imputation)\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "df_with_missing.loc[:, high_unique_features] = mean_imputer.fit_transform(\n",
    "    df_with_missing[high_unique_features]\n",
    ")\n",
    "\n",
    "# Apply rounding to nearest multiple of 7 for Var7\n",
    "df_with_missing.loc[:, numerical_divisor_7_group] = df_with_missing[\n",
    "    numerical_divisor_7_group\n",
    "].apply(lambda x: round_to_nearest_multiple(x, 7))\n",
    "\n",
    "# Apply rounding to nearest multiple of 9 for Var144 and Var65\n",
    "for var in numerical_divisor_9_group:\n",
    "    df_with_missing.loc[:, var] = df_with_missing[var].apply(\n",
    "        lambda x: round_to_nearest_multiple(x, 9)\n",
    "    )\n",
    "print(df_with_missing.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b4aebe0-5bbd-4294-ae78-54fcd76314be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate df_with_missing and df_without_missing by columns (axis=1)\n",
    "imputed_numeric_df = pd.concat([df_with_missing, df_without_missing], axis=1)\n",
    "imputed_numeric_df.to_csv('imputed_numeric_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6adda0-5796-4453-9bd6-e644f7272441",
   "metadata": {},
   "source": [
    "#### Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be70f181-acaf-4a93-aff2-238d5188d87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature summary exported to df_with_missing_cat.csv\n"
     ]
    }
   ],
   "source": [
    "df_with_missing_cat, df_without_missing_cat = split_dataframe_by_missing_values(categorical_df)\n",
    "export_feature_summary(df_with_missing_cat, 'df_with_missing_cat.csv', unique_threshold = 15)\n",
    "df_with_missing_cat.to_csv('df_with_missing_cat.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7d12476-9f03-45ed-8357-210657b5ee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_missing_cat = df_with_missing_cat.copy()\n",
    "\n",
    "mode_imputing_columns = ['Var208', 'Var218', 'Var205', 'Var203', 'Var197']\n",
    "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df_with_missing_cat[mode_imputing_columns] = mode_imputer.fit_transform(df_with_missing_cat[mode_imputing_columns])\n",
    "\n",
    "columns_to_impute_with_unknown = ['Var223', 'Var219', 'Var206']\n",
    "df_with_missing_cat[columns_to_impute_with_unknown] = df_with_missing_cat[columns_to_impute_with_unknown].fillna('Unknown')\n",
    "\n",
    "columns_to_impute_frequency = ['Var197', 'Var192', 'Var217']\n",
    "df_with_missing_cat[columns_to_impute_frequency] = mode_imputer.fit_transform(df_with_missing_cat[columns_to_impute_frequency])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f7b0a05-83a2-459d-b811-ad316896bf33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9080, 67)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_categroical_df = pd.concat([df_without_missing_cat, df_with_missing_cat], axis=1)\n",
    "full_imputed_data = pd.concat([imputed_numeric_df, imputed_categroical_df], axis=1)\n",
    "full_imputed_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e2517f-7594-41af-8464-86d00194330b",
   "metadata": {},
   "source": [
    "### Scaling And Transforming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2d723e-6d43-4306-a39b-b422021f6862",
   "metadata": {},
   "source": [
    "#### Outlier Detection and Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5eb162c-14bd-4ca8-becb-50888fe52b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame before removing features:\n",
      "(9080, 38)\n",
      "\n",
      "Features with more than 7% outliers and their outlier percentages:\n",
      "        Outlier Count  Outlier Percentage\n",
      "Var35             777            8.557269\n",
      "Var78             717            7.896476\n",
      "Var113            840            9.251101\n",
      "Var132           1632           17.973568\n",
      "\n",
      "DataFrame after removing features with more than 7% outliers:\n",
      "(9080, 34)\n"
     ]
    }
   ],
   "source": [
    "y = imputed_numeric_df['y']\n",
    "X_numeric = imputed_numeric_df.drop(columns=['y'])\n",
    "\n",
    "# Apply the outlier removal function to the features only\n",
    "outliners_cleaned_numeric_df = remove_outlier_features(X_numeric, outlier_percentage_threshold=7, iqr_threshold=3)\n",
    "\n",
    "# Concatenate the cleaned features with the target column 'y'\n",
    "outliners_cleaned_numeric_df = pd.concat([outliners_cleaned_numeric_df, y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64657eaf-c8af-489a-b988-bdbbbf14a1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_transform_cols = [\n",
    "    'Var6', 'Var13', 'Var21', 'Var24', 'Var65', 'Var74', 'Var109', 'Var119', \n",
    "    'Var125', 'Var140', 'Var44', 'Var83', 'Var85', 'Var112', 'Var123', \n",
    "    'Var143', 'Var73', 'Var76', 'Var134', 'Var133', 'Var160', 'Var163', \n",
    "    'Var173', 'Var181'\n",
    "]\n",
    "min_max_scaler_col = 'Var57'\n",
    "standart_scaler_cols = ['Var7', 'Var153']\n",
    "\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "min_max_scaler = MinMaxScaler()\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "outliners_cleaned_numeric_df[power_transform_cols] = pt.fit_transform(outliners_cleaned_numeric_df[power_transform_cols])\n",
    "outliners_cleaned_numeric_df[min_max_scaler_col] = min_max_scaler.fit_transform(outliners_cleaned_numeric_df[[min_max_scaler_col]])\n",
    "outliners_cleaned_numeric_df[standart_scaler_cols] = standard_scaler.fit_transform(outliners_cleaned_numeric_df[standart_scaler_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7c8a95-98e0-4e1b-b657-aa58658e14b9",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6520d7fe-1c49-4c77-95bb-15d2c4eb403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_categroical_df.to_csv('imputed_categroical_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecbb3aee-b5e3-4cfe-b503-fafee718346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_columns = ['Var218', 'Var208', 'Var211']\n",
    "one_hot_columns = ['Var196', 'Var205', 'Var203', 'Var223', 'Var210', 'Var227', 'Var221']\n",
    "frequency_encode_columns = ['Var207', 'Var195', 'Var219', 'Var206', 'Var226']\n",
    "high_cardinality_features = [\n",
    "    'Var192', 'Var216', 'Var199', 'Var222', \n",
    "    'Var220', 'Var198', 'Var202', 'Var217',\n",
    "    'Var228', 'Var193', 'Var212', 'Var204',\n",
    "    'Var197'\n",
    "]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "target_encoder = TargetEncoder(cols=high_cardinality_features)\n",
    "\n",
    "for col in binary_columns:\n",
    "    imputed_categroical_df[col] = label_encoder.fit_transform(imputed_categroical_df[col])\n",
    "imputed_categroical_df = pd.get_dummies(imputed_categroical_df, columns=one_hot_columns)\n",
    "\n",
    "for col in frequency_encode_columns:\n",
    "    freq = imputed_categroical_df[col].value_counts()\n",
    "    imputed_categroical_df[col] = imputed_categroical_df[col].map(freq)\n",
    "\n",
    "\n",
    "imputed_categroical_df[high_cardinality_features] = target_encoder.fit_transform(imputed_categroical_df[high_cardinality_features],\n",
    "                                                                                 full_imputed_data['y'])\n",
    "encoded_cat_df = imputed_categroical_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ef2c2b-d9cb-4c07-ab9a-7c0eb7ee38c6",
   "metadata": {},
   "source": [
    "### Model try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb87449e-b095-40e3-a1b6-51a9dabb4515",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df = pd.concat([outliners_cleaned_numeric_df, encoded_cat_df], axis=1)\n",
    "\n",
    "# Make sure 'y' is dropped from training features\n",
    "X = modeling_df.drop(columns=['y'])\n",
    "y = modeling_df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eaa75a18-c1f4-43da-a38c-cdb053997d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold Results:\n",
      "Accuracy: 0.9757709251101322\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1575\n",
      "           1       0.91      0.91      0.91       241\n",
      "\n",
      "    accuracy                           0.98      1816\n",
      "   macro avg       0.95      0.95      0.95      1816\n",
      "weighted avg       0.98      0.98      0.98      1816\n",
      "\n",
      "Fold Results:\n",
      "Accuracy: 0.9724669603524229\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1575\n",
      "           1       0.89      0.90      0.90       241\n",
      "\n",
      "    accuracy                           0.97      1816\n",
      "   macro avg       0.94      0.94      0.94      1816\n",
      "weighted avg       0.97      0.97      0.97      1816\n",
      "\n",
      "Fold Results:\n",
      "Accuracy: 0.9757709251101322\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      1574\n",
      "           1       0.89      0.93      0.91       242\n",
      "\n",
      "    accuracy                           0.98      1816\n",
      "   macro avg       0.94      0.96      0.95      1816\n",
      "weighted avg       0.98      0.98      0.98      1816\n",
      "\n",
      "Fold Results:\n",
      "Accuracy: 0.9741189427312775\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      1574\n",
      "           1       0.88      0.93      0.91       242\n",
      "\n",
      "    accuracy                           0.97      1816\n",
      "   macro avg       0.94      0.96      0.95      1816\n",
      "weighted avg       0.97      0.97      0.97      1816\n",
      "\n",
      "Fold Results:\n",
      "Accuracy: 0.9785242290748899\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1574\n",
      "           1       0.91      0.93      0.92       242\n",
      "\n",
      "    accuracy                           0.98      1816\n",
      "   macro avg       0.95      0.96      0.95      1816\n",
      "weighted avg       0.98      0.98      0.98      1816\n",
      "\n",
      "\n",
      "F1 Scores for each fold: [0.9475568903777289, 0.9404055572474193, 0.9484547521178295, 0.9452202850275497, 0.9537586142731691]\n",
      "Average F1 Score: 0.9470792198087393\n",
      "Accuracies for each fold: [0.9757709251101322, 0.9724669603524229, 0.9757709251101322, 0.9741189427312775, 0.9785242290748899]\n",
      "Average Accuracy: 0.9753303964757709\n"
     ]
    }
   ],
   "source": [
    "# Set up StratifiedKFold and SMOTE\n",
    "strat_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define model\n",
    "model = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "\n",
    "# Define F1 score as our metric\n",
    "f1_scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# List to store metrics for each fold\n",
    "f1_scores = []\n",
    "accuracies = []\n",
    "\n",
    "# Stratified K-Fold Cross-Validation\n",
    "for train_index, val_index in strat_kfold.split(X, y):\n",
    "    # Split the data\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    # Apply SMOTE to the training data\n",
    "    smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Fit the model on the balanced training data\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate and store metrics\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"Fold Results:\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Calculate and print the average scores across folds\n",
    "print(\"\\nF1 Scores for each fold:\", f1_scores)\n",
    "print(\"Average F1 Score:\", np.mean(f1_scores))\n",
    "print(\"Accuracies for each fold:\", accuracies)\n",
    "print(\"Average Accuracy:\", np.mean(accuracies))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9b5b08-be99-4ffc-af00-3f2e4915c1eb",
   "metadata": {},
   "source": [
    "### Test Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38bed6e9-d682-4ffa-a97d-e598f7735320",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('final_proj_test.csv')\n",
    "\n",
    "# Define columns to retain even if they aren't in the intersection\n",
    "columns_to_save = ['Var205', 'Var203', 'Var223', 'Var196', 'Var210', 'Var227', 'Var221']\n",
    "\n",
    "# Find common columns between modeling_df and test_data\n",
    "common_columns = modeling_df.columns.intersection(test_data.columns)\n",
    "\n",
    "# Combine common columns with columns_to_save, ensuring no duplicates\n",
    "final_columns = common_columns.union(columns_to_save)\n",
    "\n",
    "# Align test_data with these final columns, filling missing columns as needed\n",
    "test_df_aligned = test_data.reindex(columns=final_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f4913c0-461b-4d4c-b915-73a9fd776a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Var109  Var112  Var119  Var123   Var125   Var13     Var133    Var134  \\\n",
      "0    32.0    48.0   595.0    78.0   5688.0     0.0  4101810.0  807760.0   \n",
      "1   152.0    80.0  1320.0    36.0  16470.0  3864.0   689795.0  327752.0   \n",
      "2    48.0    40.0   605.0    54.0      0.0     0.0  4018990.0  796264.0   \n",
      "3    40.0    40.0   785.0   114.0  48267.0  5192.0  3744305.0   13754.0   \n",
      "4    32.0    40.0   395.0     6.0    387.0    16.0  1536535.0  104290.0   \n",
      "\n",
      "   Var140  Var143  ...     Var57    Var6  Var65  Var7  Var73  Var74  \\\n",
      "0    20.0     0.0  ...  1.667165   819.0    9.0   7.0    168    0.0   \n",
      "1  1835.0     0.0  ...  5.056398  3192.0   45.0  28.0     46  518.0   \n",
      "2     0.0     0.0  ...  3.020936   756.0    9.0   0.0     28    0.0   \n",
      "3  2905.0     0.0  ...  4.014740  3892.0   27.0  21.0     88  371.0   \n",
      "4   430.0     0.0  ...  1.277505   672.0    9.0   7.0     56    0.0   \n",
      "\n",
      "       Var76      Var81 Var83 Var85  \n",
      "0  3801816.0  186732.90  15.0   0.0  \n",
      "1  1217520.0   13126.62   5.0  38.0  \n",
      "2  2197128.0  145969.80  30.0   2.0  \n",
      "3  2857024.0   61301.70  20.0   0.0  \n",
      "4   571776.0   65348.70   0.0   2.0  \n",
      "\n",
      "[5 rows x 62 columns]\n"
     ]
    }
   ],
   "source": [
    "test_df_aligned.loc[:, low_unique_features] = mode_imputer.fit_transform(\n",
    "    test_df_aligned[low_unique_features]\n",
    ")\n",
    "\n",
    "test_df_aligned.loc[:, moderate_unique_features] = median_imputer.fit_transform(\n",
    "    test_df_aligned[moderate_unique_features]\n",
    ")\n",
    "\n",
    "test_df_aligned.loc[:, high_unique_features] = mean_imputer.fit_transform(\n",
    "    test_df_aligned[high_unique_features]\n",
    ")\n",
    "\n",
    "test_df_aligned.loc[:, numerical_divisor_7_group] = test_df_aligned[\n",
    "    numerical_divisor_7_group\n",
    "].apply(lambda x: round_to_nearest_multiple(x, 7))\n",
    "\n",
    "for var in numerical_divisor_9_group:\n",
    "    test_df_aligned.loc[:, var] = test_df_aligned[var].apply(\n",
    "        lambda x: round_to_nearest_multiple(x, 9)\n",
    "    )\n",
    "print(test_df_aligned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c56deb31-a741-4c66-bd7e-be97902691f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_aligned[mode_imputing_columns] = mode_imputer.fit_transform(test_df_aligned[mode_imputing_columns])\n",
    "\n",
    "test_df_aligned[columns_to_impute_with_unknown] = test_df_aligned[columns_to_impute_with_unknown].fillna('Unknown')\n",
    "\n",
    "test_df_aligned[columns_to_impute_frequency] = mode_imputer.fit_transform(test_df_aligned[columns_to_impute_frequency])\n",
    "\n",
    "test_df_aligned.shape\n",
    "\n",
    "imputed_test_df = test_df_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04477ec5-3242-48d0-922e-c851e26178f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame before removing features:\n",
      "(2500, 34)\n",
      "\n",
      "Features with more than 7% outliers and their outlier percentages:\n",
      "Empty DataFrame\n",
      "Columns: [Outlier Count, Outlier Percentage]\n",
      "Index: []\n",
      "\n",
      "DataFrame after removing features with more than 7% outliers:\n",
      "(2500, 34)\n"
     ]
    }
   ],
   "source": [
    "common_columns = X_numeric.columns.intersection(imputed_test_df.columns)\n",
    "\n",
    "X_numeric_test = imputed_test_df[common_columns]\n",
    "\n",
    "outliners_cleaned_test_numeric_df = remove_outlier_features(X_numeric_test, outlier_percentage_threshold=7, iqr_threshold=3)\n",
    "\n",
    "imputed_test_df[common_columns] = outliners_cleaned_test_numeric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbe79959-a279-4a8f-8905-411b891f83e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_test_df[power_transform_cols] = pt.fit_transform(imputed_test_df[power_transform_cols])\n",
    "imputed_test_df[min_max_scaler_col] = min_max_scaler.fit_transform(imputed_test_df[[min_max_scaler_col]])\n",
    "imputed_test_df[standart_scaler_cols] = standard_scaler.fit_transform(imputed_test_df[standart_scaler_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2caf5348-52e4-495a-a700-46721b327084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Binary encoding for binary columns\n",
    "for col in binary_columns:\n",
    "    imputed_test_df[col] = label_encoder.fit_transform(imputed_test_df[col])\n",
    "\n",
    "\n",
    "imputed_test_df = pd.get_dummies(imputed_test_df, columns=one_hot_columns)\n",
    "\n",
    "expected_columns = modeling_df.columns  # Columns from the training data\n",
    "imputed_test_df = imputed_test_df.reindex(columns=expected_columns, fill_value=0)\n",
    "\n",
    "# Step 3: Frequency encoding for moderate cardinality columns\n",
    "for col in frequency_encode_columns:\n",
    "    freq = imputed_test_df[col].value_counts()\n",
    "    imputed_test_df[col] = imputed_test_df[col].map(freq)\n",
    "\n",
    "# Step 4: Target encoding for high cardinality features (already fitted on training data)\n",
    "imputed_test_df[high_cardinality_features] = target_encoder.transform(imputed_test_df[high_cardinality_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7d4fbb5-3d5c-462c-b785-5ba2daa69226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 91)\n",
      "(9080, 91)\n"
     ]
    }
   ],
   "source": [
    "modelling_test_df = imputed_test_df\n",
    "print(modelling_test_df.shape)\n",
    "print(modeling_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2bf449b-918d-46a0-9edb-cbd56cfcc5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test data, ensure it aligns with the training feature set\n",
    "X_test = modelling_test_df.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "# Then, proceed with prediction\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a976c6e7-bce8-419e-ba9e-2c0b15748296",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
