{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec290c0-c953-49f0-8d57-d643137b2653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from feature_summary import export_feature_summary\n",
    "from round_to_nearest import round_to_nearest_multiple\n",
    "from split_dataset_by_missing_and_type import split_dataset_by_missing_and_type\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import hashlib\n",
    "from sklearn.preprocessing import PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1abe2ed-73c5-43c9-83ba-7b49b4b0d8bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('final_proj_data.csv')\n",
    "test_data = pd.read_csv('final_proj_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb3adc0-34a9-49b7-9d60-72abe84f2b9f",
   "metadata": {},
   "source": [
    "### Removing Not Needed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae704a41-ab37-4d91-8224-ea5cc14996ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 231)\n",
      "Columns removed: ['Var1', 'Var2', 'Var3', 'Var4', 'Var5', 'Var8', 'Var9', 'Var10', 'Var11', 'Var12', 'Var14', 'Var15', 'Var16', 'Var17', 'Var18', 'Var19', 'Var20', 'Var23', 'Var26', 'Var27', 'Var29', 'Var30', 'Var31', 'Var32', 'Var33', 'Var34', 'Var36', 'Var37', 'Var39', 'Var40', 'Var41', 'Var42', 'Var43', 'Var45', 'Var46', 'Var47', 'Var48', 'Var49', 'Var50', 'Var51', 'Var52', 'Var53', 'Var54', 'Var55', 'Var56', 'Var58', 'Var59', 'Var60', 'Var61', 'Var62', 'Var63', 'Var64', 'Var66', 'Var67', 'Var68', 'Var69', 'Var70', 'Var71', 'Var72', 'Var75', 'Var77', 'Var79', 'Var80', 'Var82', 'Var84', 'Var86', 'Var87', 'Var88', 'Var89', 'Var90', 'Var91', 'Var92', 'Var93', 'Var94', 'Var95', 'Var96', 'Var97', 'Var98', 'Var99', 'Var100', 'Var101', 'Var102', 'Var103', 'Var104', 'Var105', 'Var106', 'Var107', 'Var108', 'Var110', 'Var111', 'Var114', 'Var115', 'Var116', 'Var117', 'Var118', 'Var120', 'Var121', 'Var122', 'Var124', 'Var127', 'Var128', 'Var129', 'Var130', 'Var131', 'Var135', 'Var136', 'Var137', 'Var138', 'Var139', 'Var141', 'Var142', 'Var145', 'Var146', 'Var147', 'Var148', 'Var150', 'Var151', 'Var152', 'Var154', 'Var155', 'Var156', 'Var157', 'Var158', 'Var159', 'Var161', 'Var162', 'Var164', 'Var165', 'Var166', 'Var167', 'Var168', 'Var169', 'Var170', 'Var171', 'Var172', 'Var174', 'Var175', 'Var176', 'Var177', 'Var178', 'Var179', 'Var180', 'Var182', 'Var183', 'Var184', 'Var185', 'Var186', 'Var187', 'Var188', 'Var189', 'Var190', 'Var191', 'Var194', 'Var200', 'Var201', 'Var209', 'Var213', 'Var214', 'Var215', 'Var224', 'Var225', 'Var229', 'Var230']\n",
      "New dataset shape after column removal: (10000, 68)\n",
      "New dataset shape after removing rows with more than 30% missing columns: (9080, 68)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "column_threshold = 0.30\n",
    "\n",
    "missing_percent = data.isnull().mean()\n",
    "columns_to_drop = missing_percent[missing_percent > column_threshold].index\n",
    "cleaned_data = data.drop(columns=columns_to_drop)\n",
    "\n",
    "print(f\"Columns removed: {list(columns_to_drop)}\")\n",
    "print(f\"New dataset shape after column removal: {cleaned_data.shape}\")\n",
    "\n",
    "row_threshold = int((1 - 0.30) * cleaned_data.shape[1])\n",
    "cleaned_data = cleaned_data.dropna(thresh=row_threshold)\n",
    "print(f\"New dataset shape after removing rows with more than 30% missing columns: {cleaned_data.shape}\")\n",
    "\n",
    "cleaned_data.to_csv('fully_cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17131eb5-0cc7-4216-afa8-9674346b9351",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "941ac169-ebad-4aec-a2fd-96cbba9d6a7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summary_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# MISSING VALUES\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m features_with_missing_values_count \u001b[38;5;241m=\u001b[39m summary_df[summary_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing Value Count\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(features_with_missing_values_count)\n\u001b[1;32m      4\u001b[0m split_dataset_by_missing_and_type(summary_df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summary_df' is not defined"
     ]
    }
   ],
   "source": [
    "# MISSING VALUES\n",
    "features_with_missing_values_count = summary_df[summary_df['Missing Value Count'] > 0].shape[0]\n",
    "print(features_with_missing_values_count)\n",
    "split_dataset_by_missing_and_type(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376a3e57-ec62-49fd-804b-5730386495e4",
   "metadata": {},
   "source": [
    "#### Numerical Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e37c2a-bd5c-4337-9003-f56325298c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical continuous variables with missing values <= 100\n",
    "numerical_continuous_missing_leq_100 = ['Var6', 'Var13', 'Var21', 'Var74', 'Var81', 'Var119', 'Var125', 'Var140']\n",
    "# Perform median imputation for the specified variables\n",
    "for col in numerical_continuous_missing_leq_100:\n",
    "    if col in cleaned_data.columns:\n",
    "        median_value = cleaned_data[col].median()\n",
    "        cleaned_data[col] = cleaned_data[col].fillna(median_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc4579-e2e6-4ea5-9c57-91cbe6fd4864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical continuous variables with missing values > 100 and <= 1000\n",
    "numerical_continuous_missing_between_100_and_1000 = ['Var24', 'Var109', 'Var149']\n",
    "# Numerical continuous variables with missing values > 1000\n",
    "numerical_continuous_missing_gt_1000 = ['Var126']\n",
    "# Numerical ordinal variables with a common divisor of 7\n",
    "numerical_ordinal_divisor_7_group = ['Var7']\n",
    "# Numerical ordinal variables with a common divisor of 9\n",
    "numerical_ordinal_divisor_9_group = ['Var144', 'Var65']\n",
    "\n",
    "# Create a SimpleImputer object with the median strategy\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Apply the imputation on the selected columns\n",
    "cleaned_data[numerical_continuous_missing_gt_1000] = median_imputer.fit_transform(cleaned_data[numerical_continuous_missing_gt_1000])\n",
    "cleaned_data[numerical_continuous_missing_between_100_and_1000] = median_imputer.fit_transform(cleaned_data[numerical_continuous_missing_between_100_and_1000])\n",
    "cleaned_data[numerical_ordinal_divisor_7_group] = median_imputer.fit_transform(cleaned_data[numerical_ordinal_divisor_7_group])\n",
    "cleaned_data[numerical_ordinal_divisor_9_group] = median_imputer.fit_transform(cleaned_data[numerical_ordinal_divisor_9_group])\n",
    "\n",
    "# Apply rounding to nearest multiple of 7 for Var7\n",
    "cleaned_data[numerical_ordinal_divisor_7_group] = cleaned_data[numerical_ordinal_divisor_7_group].apply(lambda x: round_to_nearest_multiple(x, 7))\n",
    "\n",
    "# Apply rounding to nearest multiple of 9 for Var144 and Var65\n",
    "for var in numerical_ordinal_divisor_9_group:\n",
    "    cleaned_data[var] = cleaned_data[var].apply(lambda x: round_to_nearest_multiple(x, 9))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8425ad60-f52d-4571-815a-36eaffd553ac",
   "metadata": {},
   "source": [
    "#### Categorical Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4355fa14-de28-45e8-8da1-071d8c4b362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CATEGORICAL\n",
    "# Categorical continuous variables\n",
    "categorical_continuous_variables = ['Var192', 'Var197', 'Var217']\n",
    "\n",
    "# Create SimpleImputer object with the most frequent strategy\n",
    "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "cleaned_data[categorical_continuous_variables] = mode_imputer.fit_transform(cleaned_data[categorical_continuous_variables])\n",
    "\n",
    "# Categorical variables with few unique values and up to 345 missing\n",
    "categorical_variables_few_unique_missing_leq_345 = ['Var203', 'Var205', 'Var218', 'Var208']\n",
    "cleaned_data[categorical_variables_few_unique_missing_leq_345] = mode_imputer.fit_transform(cleaned_data[categorical_variables_few_unique_missing_leq_345])\n",
    "\n",
    "# Categorical variables with many unique values and little missing\n",
    "categorical_variables_many_unique_missing_low = ['Var206']\n",
    "# Apply the imputation\n",
    "constant_imputer = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "cleaned_data[categorical_variables_many_unique_missing_low] = constant_imputer.fit_transform(cleaned_data[categorical_variables_many_unique_missing_low])\n",
    "\n",
    "# Categorical variables with many unique values and many missing\n",
    "categorical_variables_many_unique_missing_high = ['Var219']\n",
    "# Apply the KNN imputation on categorical variables with many unique values\n",
    "cleaned_data[categorical_variables_many_unique_missing_high] = constant_imputer.fit_transform(cleaned_data[categorical_variables_many_unique_missing_high])\n",
    "\n",
    "# Categorical variables with few unique values and many missing\n",
    "categorical_variables_few_unique_missing_high = ['Var223']\n",
    "# Apply most frequent imputation\n",
    "cleaned_data[categorical_variables_few_unique_missing_high] = mode_imputer.fit_transform(cleaned_data[categorical_variables_few_unique_missing_high])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e120c0-f7fd-4d86-ac6f-b5a7c57f41ef",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b65168-a081-4c5d-8456-5653e6588b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data = cleaned_data\n",
    "imputed_data.to_csv('fully_imputed_data.csv', index=False)\n",
    "export_feature_summary(cleaned_data, 'feature_summary.csv', unique_threshold = 40)\n",
    "summary_df = pd.read_csv('feature_summary.csv')\n",
    "split_dataset_by_missing_and_type(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc5e234-f2b3-48ff-91e8-7e474fa5d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_features_names = ['Var208', 'Var218', 'Var218']\n",
    "label_encoders = {}\n",
    "for feature in binary_features_names:\n",
    "    label_encoders[feature] = LabelEncoder()\n",
    "    imputed_data[feature] = label_encoders[feature].fit_transform(imputed_data[feature])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed95d70-6884-46f7-9b5c-70ca074bcff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoding_names = ['Var196', 'Var205', 'Var223', 'Var203', 'Var210', 'Var221', 'Var227', 'Var207']\n",
    "# Apply one-hot encoding using pd.get_dummies\n",
    "imputed_data_one_hot_encoded = pd.get_dummies(imputed_data, columns=one_hot_encoding_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca157ec6-fbe3-477b-bfaf-2379b233ab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "freequency_encoding_names = ['Var219', 'Var206', 'Var226', 'Var228', 'Var193', 'Var212', 'Var204']\n",
    "# Apply frequency encoding to each feature\n",
    "for feature in freequency_encoding_names:\n",
    "    frequency_map = imputed_data[feature].value_counts() / len(imputed_data)\n",
    "    imputed_data[feature] = imputed_data[feature].map(frequency_map)\n",
    "\n",
    "# Display the first few rows of the dataset after frequency encoding\n",
    "print(imputed_data[freequency_encoding_names].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cd675a-3e7c-43a3-a6a5-b6a87bbc5814",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoding_names = ['Var197', 'Var192']\n",
    "# Apply target encoding\n",
    "for feature in target_encoding_names:\n",
    "    # Create a mapping from category to mean target value\n",
    "    target_mean = imputed_data.groupby(feature)['y'].mean()\n",
    "    imputed_data[feature] = imputed_data[feature].map(target_mean)\n",
    "\n",
    "# Display the first few rows of the dataset after target encoding\n",
    "print(imputed_data[target_encoding_names].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d105c56a-4f72-46da-93b2-32498786413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothning_target_encoding_names = ['Var220', 'Var198', 'Var216', 'Var199', 'Var222']\n",
    "# Assuming your target variable is called 'y' (replace 'y' with the actual target column name)\n",
    "global_mean = imputed_data['y'].mean()\n",
    "\n",
    "# Define a smoothing factor (you can adjust this value)\n",
    "alpha = 10\n",
    "\n",
    "# Apply smoothing target encoding\n",
    "for feature in smoothning_target_encoding_names:\n",
    "    # Calculate category mean and the count of occurrences\n",
    "    category_stats = imputed_data.groupby(feature)['y'].agg(['mean', 'count'])\n",
    "    \n",
    "    # Apply the smoothing formula\n",
    "    smooth = (category_stats['count'] * category_stats['mean'] + alpha * global_mean) / (category_stats['count'] + alpha)\n",
    "    \n",
    "    # Map the smoothed values back to the original feature\n",
    "    imputed_data[feature] = imputed_data[feature].map(smooth)\n",
    "\n",
    "# Display the first few rows of the dataset after smoothing target encoding\n",
    "print(imputed_data[smoothning_target_encoding_names].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c406f11d-da58-4082-93ff-6444748a271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_encoding_names = ['Var202', 'Var217']\n",
    "# Define the number of buckets for the hash function (adjust this value as needed)\n",
    "n_buckets = 100\n",
    "\n",
    "# Function to apply hashing encoding\n",
    "def hash_encode(value, n_buckets):\n",
    "    return int(hashlib.md5(str(value).encode()).hexdigest(), 16) % n_buckets\n",
    "\n",
    "# Apply hashing encoding to each feature\n",
    "for feature in hashing_encoding_names:\n",
    "    imputed_data[feature + '_hashed'] = imputed_data[feature].apply(lambda x: hash_encode(x, n_buckets))\n",
    "\n",
    "# Display the first few rows of the dataset after hashing encoding\n",
    "print(imputed_data[[feature + '_hashed' for feature in hashing_encoding_names]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9b360b-c102-4b66-94a7-fbb0528739f6",
   "metadata": {},
   "source": [
    "### Normalising Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2636d2f-bb12-45e7-b588-266be86cfda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# Function to apply Winsorization to numeric features\n",
    "def winsorize_features(df, features, limits=(0.05, 0.05)):\n",
    "    # Winsorize each feature with the given limits (5% on both sides by default)\n",
    "    for feature in features:\n",
    "        df[feature] = winsorize(df[feature], limits=limits)\n",
    "    return df\n",
    "\n",
    "# Apply Winsorization to the numeric features with 5% limits on both sides\n",
    "winsorized_data = winsorize_features(imputed_data, numeric_features, limits=(0.05, 0.05))\n",
    "\n",
    "# Check the result\n",
    "print(f\"Data shape after Winsorizing: {winsorized_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e85948b-ce34-41bf-be58-3b07b094ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaling = ['Var173']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "imputed_data[min_max_scaling] = scaler.fit_transform(winsorized_data[min_max_scaling])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad57bdd2-42e4-4aa3-a6da-97f3d4131b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_scale = ['Var143', 'Var7', 'Var181', 'Var44']\n",
    "imputed_data[features_to_scale] = scaler.fit_transform(winsorized_data[features_to_scale])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4902da44-bcbe-47e8-a5fe-d1a64e0c85ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features to scale using StandardScaler\n",
    "features_to_standardize = ['Var144', 'Var35', 'Var78', 'Var65', 'Var132']\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply the scaler to the selected columns\n",
    "imputed_data[features_to_standardize] = scaler.fit_transform(winsorized_data[features_to_standardize])\n",
    "\n",
    "# Check the standardized data\n",
    "print(winsorized_data[features_to_standardize].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2f09c-589c-4b14-a615-04ad34fef1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of features to apply PowerTransformer\n",
    "features_to_transform = [\n",
    "    'Var126', 'Var24', 'Var85', 'Var83', 'Var109', \n",
    "    'Var73', 'Var112', 'Var25', 'Var123', 'Var160', 'Var74',\n",
    "    'Var21', 'Var22', 'Var119', 'Var6', 'Var140', 'Var13', 'Var28', \n",
    "    'Var125', 'Var149', 'Var163', 'Var76', 'Var38', 'Var134', 'Var133', \n",
    "    'Var57', 'Var153', 'Var81', 'Var113'\n",
    "]\n",
    "\n",
    "# Initialize PowerTransformer (Yeo-Johnson is used by default)\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "# Apply PowerTransformer to the selected features\n",
    "imputed_data[features_to_transform] = pt.fit_transform(winsorized_data[features_to_transform])\n",
    "\n",
    "# Check the result (first few rows) after transformation\n",
    "winsorized_data[features_to_transform].head()\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply StandardScaler to the transformed features\n",
    "winsorized_data[features_to_transform] = scaler.fit_transform(winsorized_data[features_to_transform])\n",
    "\n",
    "# Check the result after scaling\n",
    "winsorized_data[features_to_transform].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef38b4a-b407-4980-b668-3c0f83e2cfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "winsorized_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6468be8-c739-4ac1-842b-877d9596518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Підрахунок кількості кожного класу в цільовій змінній y\n",
    "class_counts = winsorized_data['y'].value_counts()\n",
    "\n",
    "# Відсотковий розподіл кожного класу\n",
    "class_percentage = winsorized_data['y'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Виведення результатів\n",
    "print(f\"Кількість класів:\\n{class_counts}\")\n",
    "print(f\"\\nВідсоткове співвідношення класів:\\n{class_percentage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d672bbf-5956-414f-affa-0721ec13f6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
